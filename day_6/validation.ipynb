{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection & Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are going to look at strategies to divide your dataset in order to perform model selection and testing using subsets of data in ways that do not create bias in your measurement of model performance.\n",
    "\n",
    "We are going to use a dataset which comes from a study done to try to use sonar signals to differentiate between a mine (simulated using a metal cylinder) and a rock.  We have 208 observations (sonar readings), and each observation has 60 features (energy in a particular frequency band summed over a set period of time) and a target value (rock 'R' or mine 'M').  Our goal will be to build a model which can use the sonar readings to predict whether the object is a mine or rock.\n",
    "\n",
    "Details on the dataset can be found [here](https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we know we need\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(208, 61)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4       5       6       7       8   \\\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "\n",
       "       9   ...      51      52      53      54      55      56      57  \\\n",
       "0  0.2111  ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084   \n",
       "1  0.2872  ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049   \n",
       "2  0.6194  ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164   \n",
       "3  0.1264  ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044   \n",
       "4  0.4459  ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048   \n",
       "\n",
       "       58      59  60  \n",
       "0  0.0090  0.0032   R  \n",
       "1  0.0052  0.0044   R  \n",
       "2  0.0095  0.0078   R  \n",
       "3  0.0040  0.0117   R  \n",
       "4  0.0107  0.0094   R  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv'\n",
    "data = pd.read_csv(url, header=None)\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have 208 observations (sonar readings), and each observation has 60 features (energy in a particular frequency band summed over a set period of time) and a target value (rock 'R' or mine 'M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((208, 60), (208,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create feature matrix using the first 60 columns as the features\n",
    "X = data.iloc[:,:60]\n",
    "\n",
    "# Create target vector from the last column\n",
    "y = data.iloc[:,60]\n",
    "\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation: Splitting data into training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we split the data into a training and a test set, we use only the training data to fit the model.  Once we have trained our model, we use it to generate predictions on the test set data and calculate error metrics based on those predictions.  This ensures that we are evaluating the model based on its ability to create predictions for data it has not seen before, which is more representative of what the model will need to do in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets, using 85% of the data for training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, random_state=0,test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use a multi-layer perceptron, a form of neural network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Create an instance of the MLPClassifier algorithm and set the hyperparameter values\n",
    "model = MLPClassifier(hidden_layer_sizes=(100,50,10),activation='tanh',\n",
    "                      solver='sgd',learning_rate_init=0.01,max_iter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='tanh', hidden_layer_sizes=(100, 50, 10),\n",
       "              learning_rate_init=0.01, max_iter=2000, solver='sgd')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model using only the training set data\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of our classifier on the test set is 0.875\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of our model using the test set data\n",
    "preds = model.predict(X_test)\n",
    "acc_test = sum(preds==y_test)/len(y_test)\n",
    "print('Accuracy of our classifier on the test set is {:.3f}'.format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we want to compare different models (for example, evaluate different algorithms or fine-tune our hyperparameters)?  Can we use the same strategy of training each model on the training data and then comparing their performance on the test set to select the best model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection: Splitting data into training, validation and test sets\n",
    "When we are seeking to optimize models by tuning hyperparameters or comparing different algorithms, it is a best practice to do so by comparing the performance of your model options using a \"validation\" set, and then reserve use of the test set to evaluate the performance of the final model you have selected.  To utilize this approach we must split our data three ways to create a training set, validation set, and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((149, 60), (27, 60), (32, 60))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data first into training and testing to get test set using 15% of data for test\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, random_state=0,test_size=0.15)\n",
    "\n",
    "# Now split the training set again into training and validation, using 15% of training data for validation\n",
    "X_train,X_val,y_train,y_val = train_test_split(X_train,y_train,random_state=0,test_size=0.15)\n",
    "\n",
    "# Verify we have what we expect in each set\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare two different models and determine which one gives us better performance.  Both of the models below are Multilayer Perceptron (simple neural network) models with different shapes.  Do not worry about what these are or how they work for now, we will get to that in a later lesson.  For now you can treat them as black box models to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of each model we want to evaluate\n",
    "\n",
    "model1 = MLPClassifier(hidden_layer_sizes=(100,50,10),activation='tanh',\n",
    "                      solver='sgd',learning_rate_init=0.01,max_iter=2000)\n",
    "\n",
    "model2 = MLPClassifier(hidden_layer_sizes=(100,50),activation='relu',\n",
    "                      solver='sgd',learning_rate_init=0.01,max_iter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of model1 on the validation set is 0.778\n",
      "Accuracy of model2 on the validation set is 0.852\n"
     ]
    }
   ],
   "source": [
    "# Compare the performance of the two models using the validation set\n",
    "model1.fit(X_train,y_train)\n",
    "val_preds_model1 = model1.predict(X_val)\n",
    "acc_val_model1 = sum(val_preds_model1==y_val)/len(y_val)\n",
    "\n",
    "model2.fit(X_train,y_train)\n",
    "val_preds_model2 = model2.predict(X_val)\n",
    "acc_val_model2 = sum(val_preds_model2==y_val)/len(y_val)\n",
    "\n",
    "print('Accuracy of model1 on the validation set is {:.3f}'.format(acc_val_model1))\n",
    "print('Accuracy of model2 on the validation set is {:.3f}'.format(acc_val_model2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the performance of our two models on the validation set, we would select model2 to use as our model.  Let's now use the test set to evaluate its performance on data it has not yet seen so we can state a more accurate performance level.  Since we are using the test set to evaluate performance, we can now train it on a combination of the train and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of our model on the test set is 0.938\n"
     ]
    }
   ],
   "source": [
    "# Train our selected model on the training plus validation sets\n",
    "model2.fit(pd.concat([X_train,X_val]),pd.concat([y_train,y_val]))\n",
    "\n",
    "# Evaluate its performance on the test set\n",
    "preds_test = model2.predict(X_test)\n",
    "acc_test = sum(preds_test==y_test)/len(y_test)\n",
    "print('Accuracy of our model on the test set is {:.3f}'.format(acc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection: Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common approach to comparing and optimizing models is to use cross-validation rather than a single validation set to compare model performace.  We will then select the better model based on the cross-validation performance and use the test set to determine its performance.\n",
    "\n",
    "Let's do another example comparing two different models (model2 and model3) using cross-validation, with accuracy as our evaluation metric.  Don't worry about the details of model2 and model3 for now, you can treat them as black box models we want to compare and select the better one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set aside a test set and use the remainder for training and cross-validation\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, random_state=0,test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(hidden_layer_sizes=(100, 50), learning_rate_init=0.01,\n",
      "              max_iter=1000, solver='sgd')\n",
      "Fold accuracy: 0.933\n",
      "Fold accuracy: 0.600\n",
      "Fold accuracy: 0.933\n",
      "Fold accuracy: 0.933\n",
      "Fold accuracy: 0.800\n",
      "Fold accuracy: 0.600\n",
      "Fold accuracy: 0.867\n",
      "Fold accuracy: 0.667\n",
      "Fold accuracy: 0.867\n",
      "Fold accuracy: 0.929\n",
      "Mean cross-validation accuracy across all folds is 0.813 \n",
      "\n",
      "KNeighborsClassifier()\n",
      "Fold accuracy: 0.733\n",
      "Fold accuracy: 0.533\n",
      "Fold accuracy: 0.933\n",
      "Fold accuracy: 0.800\n",
      "Fold accuracy: 0.667\n",
      "Fold accuracy: 0.600\n",
      "Fold accuracy: 0.600\n",
      "Fold accuracy: 0.667\n",
      "Fold accuracy: 0.867\n",
      "Fold accuracy: 0.786\n",
      "Mean cross-validation accuracy across all folds is 0.719 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the KFold class\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "# Set up the two models we want to compare: a neural network model and a KNN model\n",
    "model2 = MLPClassifier(hidden_layer_sizes=(100,50),activation='relu',\n",
    "                      solver='sgd',learning_rate_init=0.01,max_iter=1000)\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model3 = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# For each model, use K-folds cross validation to calculate the CV accuracy\n",
    "\n",
    "for model in [model2,model3]:\n",
    "    print(model)\n",
    "    \n",
    "    acc_folds = [] # List to hold the validation fold accuracy at each iteration\n",
    "    # For each iteration, train the model on the training folds and calculate the accuracy on the validation folds\n",
    "    for (train_idx,val_idx) in kf.split(X=X_train,y=y_train):\n",
    "\n",
    "        # Split training and validation sets for each fold\n",
    "        X_fold_train, X_fold_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_fold_train, y_fold_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "\n",
    "        # Fit model to the training data for this iteration\n",
    "        model.fit(X_fold_train,y_fold_train)\n",
    "\n",
    "        # Get predictions for the validation fold and calculate accuracy\n",
    "        preds = model.predict(X_fold_val)\n",
    "        acc_val = sum(preds==y_fold_val)/len(y_fold_val)\n",
    "        \n",
    "        print('Fold accuracy: {:.3f}'.format(acc_val))\n",
    "\n",
    "        # Add the accuracy score to the acc_folds list\n",
    "        acc_folds.append(acc_val)\n",
    "        \n",
    "    # Calculate the mean accuracy across all iterations\n",
    "    mean_acc = np.mean(acc_folds)\n",
    "\n",
    "    print('Mean cross-validation accuracy across all folds is {:.3f} \\n'.format(mean_acc))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the cross-validation accuracy of model2 is higher than model3, so we will use model2 as our final model.  Let's now evaluate the performance of model2 on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of our model on the test set is 0.875\n"
     ]
    }
   ],
   "source": [
    "# Train our selected model on the full training set\n",
    "model2.fit(X_train,y_train)\n",
    "    \n",
    "# Evaluate its performance on the test set\n",
    "preds_test = model2.predict(X_test)\n",
    "acc_test = sum(preds_test==y_test)/len(y_test)\n",
    "print('Accuracy of our model on the test set is {:.3f}'.format(acc_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
