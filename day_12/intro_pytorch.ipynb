{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='https://ai.meng.duke.edu'> = <img align=\"left\" style=\"padding-top:10px;\" src=https://storage.googleapis.com/aipi_datasets/Duke-AIPI-Logo.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pytorch and torchvision from the command line if you have not already done so\n",
    "# pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic object used in PyTorch is the 'Tensor' which is equivalent to 'ndarray' in Numpy. Similar to Numpy, there are multiple types of Tensors, e.g. Float, Double, Int, Long, etc. Generally we will use FloatTensors, and it is the default type for most functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2.],\n",
       "        [3., 4.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor manually\n",
    "x_manual = torch.tensor([[1.0, 2.0], [3.0, 4.0]])\n",
    "x_manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "tensor([[0.4963, 0.7682, 0.0885, 0.1320],\n",
      "        [0.3074, 0.6341, 0.4901, 0.8964],\n",
      "        [0.4556, 0.6323, 0.3489, 0.4017]])\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones(3,4)\n",
    "print(x_ones)\n",
    "\n",
    "x_zeros = torch.zeros(3,4)\n",
    "print(x_zeros)\n",
    "\n",
    "x_uniform = torch.rand(3,4)\n",
    "print(x_uniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3.]\n",
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor from a NumPy array\n",
    "np_array = np.array([1., 2., 3.], dtype=np.float32)\n",
    "print(np_array)\n",
    "torch_tensor = torch.from_numpy(np_array)\n",
    "print(torch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0223, 0.1689, 0.2939])\n",
      "[0.02232575 0.16885895 0.29388845]\n"
     ]
    }
   ],
   "source": [
    "# Create a NumPy array from a tensor\n",
    "another_tensor = torch.rand(3)\n",
    "print(another_tensor)\n",
    "another_np_array = another_tensor.numpy()\n",
    "print(another_np_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5185, 0.6977, 0.8000],\n",
      "        [0.1610, 0.2823, 0.6816],\n",
      "        [0.9152, 0.3971, 0.8742]])\n",
      "tensor([0.6977, 0.2823, 0.3971])\n",
      "tensor([[0.5185, 0.6977, 0.8000],\n",
      "        [0.1610, 0.2823, 0.6816]])\n"
     ]
    }
   ],
   "source": [
    "# Use indexing to get slices from a tensor\n",
    "A = torch.rand(3,3)\n",
    "print(A)\n",
    "print(A[:, 1])\n",
    "print(A[:2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6892, 0.7036, 0.9845],\n",
      "        [0.2443, 1.1150, 1.0965],\n",
      "        [1.0474, 1.4583, 0.4196]])\n",
      "tensor([[0.1132, 0.0833, 0.0302],\n",
      "        [0.0075, 0.1722, 0.2700],\n",
      "        [0.2265, 0.4905, 0.0429]])\n",
      "tensor([[0.9355, 1.0787, 0.6453],\n",
      "        [0.3255, 0.3742, 0.2261],\n",
      "        [0.4069, 1.0051, 0.7265]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.rand(3,3)\n",
    "B = torch.rand(3,3)\n",
    "\n",
    "# Add tensors together\n",
    "print(A+B)\n",
    "\n",
    "# Element-wise multiply tensors\n",
    "print(A*B)\n",
    "\n",
    "# Matrix-Matrix multiplication of tensors\n",
    "print(torch.mm(A,B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    cuda = True\n",
    "else:\n",
    "    cuda = False\n",
    "cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5846, 0.0332, 0.1387,  ..., 0.9534, 0.2357, 0.3334],\n",
       "        [0.8576, 0.6120, 0.8924,  ..., 0.3778, 0.3465, 0.4203],\n",
       "        [0.1008, 0.9075, 0.2329,  ..., 0.8757, 0.6707, 0.0709],\n",
       "        ...,\n",
       "        [0.9011, 0.0352, 0.5583,  ..., 0.3135, 0.2705, 0.3187],\n",
       "        [0.0967, 0.0548, 0.4999,  ..., 0.4541, 0.5116, 0.8959],\n",
       "        [0.6136, 0.4996, 0.0217,  ..., 0.3558, 0.1079, 0.0682]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Attach a variable to the GPU\n",
    "mat_gpu = torch.rand(5000, 5000)\n",
    "if cuda:\n",
    "    mat_gpu = mat_gpu.cuda()\n",
    "mat_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autograd\n",
    "The key thing that PyTorch provides us is its Autograd capability which provides automatic differentiation. A Tensor keeps its value and the gradient with respect to this Tensor value. Almost all of built-in operations in PyTorch supports automatic differentiation. To use it we can call `.backward()` on a computation graph, e.g. neural network, after we finish our computation on the graph, and we can automatically get the accumulated gradient for each Tensor (which has specified `requires_grad=True`) in the computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = tensor(2.)\n",
      "w = tensor(0.5000, requires_grad=True)\n",
      "b = tensor(0.1000, requires_grad=True)\n",
      "y = tensor(1.1000, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=False)\n",
    "w = torch.tensor(0.5, requires_grad=True)\n",
    "b = torch.tensor(0.1, requires_grad=True)\n",
    "print('x =',x)\n",
    "print('w =',w)\n",
    "print('b =',b)\n",
    "\n",
    "# Define a computational graph\n",
    "y = w*x + b #y = 0.5x + 0.1 and y(2) = 1.1\n",
    "print('y =',y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the gradient (derivative) of the above function y=wx+b with respect to our weight w and bias term b.  We can calculate them manually:\n",
    "\n",
    "For w:\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial w} = \\frac{\\partial}{\\partial w}\\left(wx + b\\right) = x\\\\\n",
    "\\text{and}\\\\\n",
    "\\displaystyle \\frac{\\partial y}{\\partial w}\\Bigr|_{x=2} = 2 \n",
    "$$\n",
    "For b:\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial b} = \\frac{\\partial}{\\partial b}\\left(wx + b\\right) = 1\\\\\n",
    "\\text{and}\\\\\n",
    "\\displaystyle \\frac{\\partial y}{\\partial b}\\Bigr|_{x=2} = 1 \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient with respect to w: tensor(2.)\n",
      "Gradient with respect to b: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Compute gradients of y with respect to each variable x,w,b\n",
    "y.backward()\n",
    "\n",
    "print('Gradient with respect to w:',w.grad)\n",
    "print('Gradient with respect to b:',b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-forward neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate a feed-forward neural network in PyTorch, we will use the University of Wisconsin breast cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data=load_breast_cancer(as_frame=True)\n",
    "X,y=data.data,data.target\n",
    "# Since the default in the file is 0=malignant 1=benign we want to reverse these\n",
    "y=(y==0).astype(int)\n",
    "X,y= np.array(X),np.array(y)\n",
    "\n",
    "# Let's set aside a test set and use the remainder for training and cross-validation\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, random_state=0,test_size=0.2)\n",
    "\n",
    "# Let's scale our data to help the algorithm converge faster\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Set up dataloaders for our data\n",
    "The first step is to set up the dataloaders to feed our data into the model.  We first create a `TensorDataset` for our training data and our test data.  Then we create `DataLoaders` for the training and test data which allow us to iteratively feed the data into our model in batches (called \"mini-batches\") of a size that we can specify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "torch.manual_seed(0)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(0)\n",
    "\n",
    "# Convert training and test data to TensorDatasets\n",
    "trainset = TensorDataset(torch.from_numpy(X_train_scaled.astype('float32')), \n",
    "                         torch.from_numpy(y_train.astype('float32')))\n",
    "testset = TensorDataset(torch.from_numpy(X_test_scaled.astype('float32')), \n",
    "                        torch.from_numpy(y_test.astype('float32')))\n",
    "\n",
    "# Create Dataloaders for our training and test data to allow us to iterate over minibatches \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=8, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Define our neural network architecture\n",
    "Next, we will define a model, feed-forward neural network for this chapter..\n",
    "For simplicity, we will use 3-layer, 2 hidden layers and 1 hidden-to-output layer, feed-forward net. Each layer is a fully-connected layer where the module `torch.nn.Linear` is the implementation of it. Also, we will apply ReLU activation for each layer.\n",
    "\n",
    "Basically, we are required to define a member method of `forward(self, x)` when we define a class for any customized network. It represents a forward pass of a computational graph and a backward pass (back-propagation) with automatic differentiation will be performed later based on this forward definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FeedForwardNet(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden1, n_hidden2, n_output):\n",
    "        super(FeedForwardNet, self).__init__()\n",
    "        self.hidden1 = nn.Linear(n_input, n_hidden1)\n",
    "        self.hidden2 = nn.Linear(n_hidden1, n_hidden2)\n",
    "        self.out = nn.Linear(n_hidden2, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = torch.sigmoid(self.out(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our neural network\n",
    "net = FeedForwardNet(n_input=X_train_scaled.shape[1], n_hidden1=50, n_hidden2=20, n_output=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, PyTorch gives us an easy way to define a model layer by layer using `nn.Sequential()` rather than creating a model class as we did above.  Here we define the same model as above much more simply:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a feed-forward network\n",
    "n_input = X_train_scaled.shape[1]\n",
    "n_hidden1 = 50\n",
    "n_hidden2 = 20\n",
    "n_output = 1\n",
    "\n",
    "net = nn.Sequential(nn.Linear(n_input, n_hidden1),  # hidden layer 1\n",
    "                      nn.ReLU(), # hidden layer 1 activation\n",
    "                      nn.Linear(n_hidden1, n_hidden2), # hidden layer 2\n",
    "                      nn.ReLU(), # hidden layer 2 activation\n",
    "                      nn.Linear(n_hidden2, n_output), # output layer\n",
    "                      nn.Sigmoid()) # use sigmoid as output activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define a cost / loss function and optimizer\n",
    "We will use Binary Cross Entropy as our loss function, which is usually named `criterion` in PyTorch.  For our optimizer we will use SGD.\n",
    "\n",
    "When we create an optimizer in PyTorch, we need to pass in the parameters that we want to optimize (train), which are our weights. We can retrieve all trainable parameters of the model by calling `model.parameters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define the cost / loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Define the method of updating the weights each iteration (e.g. gradient descent)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Train the model\n",
    "To train our model, we perform the following four steps in a loop, using one input mini-batch at a time:  \n",
    "    1) Make a forward pass through the network to calculate the network output  \n",
    "    2) Use the network output to calculate the cost/loss  \n",
    "    3) Calculate the gradient of the cost/loss with respect to the weights by performing a backward pass through the network with loss.backward()  \n",
    "    4) Update the weights by taking a step with the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_iter = 200\n",
    "cost = []\n",
    "for epoch in range(num_iter):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(trainloader):\n",
    "        \n",
    "        # Get the inputs X and labels y for the minibatch\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the gradients of the weights each iteration\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Calculate the predictions and the cost/loss\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Use autograd to calculate the gradient of the cost with respect to each weight\n",
    "        loss.backward()\n",
    "        \n",
    "        # Use the optimizer to do the weights update\n",
    "        optimizer.step()\n",
    "\n",
    "        # Add the loss to running loss for the epoch\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    cost.append(running_loss)\n",
    "        \n",
    "plt.plot(cost)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost/loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Evaluate the model on the validation/test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy is 0.974\n"
     ]
    }
   ],
   "source": [
    "# Turn off autograd\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    # Set up lists to store true and predicted values\n",
    "    y_true = y_test.tolist\n",
    "    test_preds = []\n",
    "\n",
    "    # Calculate the predictions on the test set and add to list\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        outputs = net(inputs)\n",
    "        test_preds.extend(outputs.data.numpy().flatten().tolist())\n",
    "\n",
    "    # Convert the predictions to discrete and calculate the accuracy\n",
    "    test_preds = np.round(test_preds)\n",
    "    test_acc = np.sum(test_preds==y_test)/len(y_test)\n",
    "    print('Test set accuracy is {:.3f}'.format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
